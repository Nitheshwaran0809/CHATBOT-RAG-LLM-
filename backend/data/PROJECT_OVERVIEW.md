# ChatGPT Clone with RAG System - Project Overview

## ğŸš€ Project Description
This is a production-ready ChatGPT clone with integrated RAG (Retrieval-Augmented Generation) system for code generation assistance. The system provides both general conversational AI and context-aware code assistance.

## ğŸ—ï¸ Architecture Overview

### Tech Stack
- **Backend**: FastAPI (Python)
- **Frontend**: Streamlit
- **LLM**: Groq API (llama-3.1-8b-instant)
- **Vector Database**: ChromaDB
- **Containerization**: Docker & Docker Compose
- **Embeddings**: Hash-based fallback system

### System Components
1. **Frontend (Streamlit)**: User interface with chat functionality
2. **Backend (FastAPI)**: API server with RAG pipeline
3. **ChromaDB**: Vector storage for document embeddings
4. **Groq LLM**: Language model for chat responses
5. **RAG Pipeline**: Document processing and retrieval system

## ğŸ“ Project Structure
```
chatgpt/
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py         # FastAPI application entry point
â”‚   â”‚   â”œâ”€â”€ config.py       # Configuration settings
â”‚   â”‚   â”œâ”€â”€ routers/        # API route handlers
â”‚   â”‚   â”œâ”€â”€ services/       # Business logic services
â”‚   â”‚   â””â”€â”€ models/         # Pydantic models
â”‚   â”œâ”€â”€ data/               # Manual data directory for RAG
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ Dockerfile         # Backend container config
â”œâ”€â”€ frontend/               # Streamlit frontend
â”‚   â”œâ”€â”€ app.py             # Main Streamlit app
â”‚   â”œâ”€â”€ components/        # UI components
â”‚   â”œâ”€â”€ services/          # Frontend services
â”‚   â””â”€â”€ styles/            # CSS styling
â”œâ”€â”€ docker-compose.yml     # Container orchestration
â”œâ”€â”€ .env                   # Environment variables
â””â”€â”€ chroma_viewer.html     # ChromaDB admin interface
```

## ğŸ”§ Key Features

### Chat Modes
1. **General Chat Mode**: Standard conversational AI
2. **Code Assistant Mode**: RAG-enhanced with project context

### RAG System
- Document ingestion from `/backend/data/` directory
- Intelligent chunking by file type
- Vector similarity search
- Context-aware response generation

### Optimizations
- Lightweight build (removed heavy ML dependencies)
- Hash-based embeddings fallback
- Manual data setup instead of file upload
- Fast container startup (~2 minutes)

## ğŸš€ Deployment
- Dockerized with docker-compose
- Frontend: http://localhost:8501
- Backend API: http://localhost:8000
- API Documentation: http://localhost:8000/docs
- ChromaDB Viewer: chroma_viewer.html

## ğŸ”‘ Environment Configuration
- GROQ_API_KEY: Required for LLM functionality
- Model: llama-3.1-8b-instant (current default)
- ChromaDB: Persistent storage in Docker volume

## ğŸ“Š Current Status
- âœ… Core chat functionality working
- âœ… RAG pipeline implemented
- âœ… Manual data setup configured
- âœ… Optimized for fast builds
- âœ… Clean, minimal UI
